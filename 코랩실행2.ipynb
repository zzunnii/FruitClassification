{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":318},"id":"JPeHaCoX4zmr","executionInfo":{"status":"error","timestamp":1733657789602,"user_tz":-540,"elapsed":9615,"user":{"displayName":"이성준","userId":"09916915111798806291"}},"outputId":"7f8f8505-f67d-4804-b980-cad875b0bef7"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","from multiprocessing import Pool\n","from functools import partial\n","import subprocess\n","\n","def unzip_file(zip_info):\n","    zip_file_path, extract_path = zip_info\n","    os.makedirs(extract_path, exist_ok=True)\n","\n","    # subprocess를 사용하여 unzip 명령어 실행\n","    subprocess.run(['unzip', '-q', zip_file_path, '-d', extract_path], check=True)\n","    return f'{zip_file_path} 압축이 {extract_path}에 성공적으로 해제되었습니다.'\n","\n","# 압축 해제할 경로와 파일 목록\n","zip_files = [\n","    (\"/content/drive/Shareddrives/Data/dataset.zip\", \"/tmp\"),\n","]\n","\n","# 병렬 처리로 압축 해제\n","if __name__ == '__main__':\n","    with Pool(processes=len(zip_files)) as pool:  # 파일 개수만큼 프로세스 생성\n","        results = pool.map(unzip_file, zip_files)\n","\n","    # 결과 출력\n","    for result in results:\n","        print(result)"],"metadata":{"id":"cJcVp_Ll41yf","executionInfo":{"status":"aborted","timestamp":1733657789603,"user_tz":-540,"elapsed":2,"user":{"displayName":"이성준","userId":"09916915111798806291"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1PLv7YqGAu-R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 다시\n"],"metadata":{"id":"2oeBWLO50xXP"}},{"cell_type":"markdown","source":["# 전처리 셀 1"],"metadata":{"id":"DJrS0mw1Tddt"}},{"cell_type":"code","source":["import os, cv2, numpy as np\n","from pathlib import Path\n","import shutil\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import random\n","from sklearn.model_selection import train_test_split\n","\n","# 경로 설정\n","TRAIN_DIR = '/tmp/train_zip/train'\n","TEST_DIR = '/tmp/test_zip/test'\n","PREPROCESS_DIR = '/tmp/preprocess_data'\n","TARGET_SIZE = 256\n","\n","random.seed(42)  # 재현성 있는 랜덤\n","\n","def create_directories():\n","    if os.path.exists(PREPROCESS_DIR):\n","        shutil.rmtree(PREPROCESS_DIR)\n","    os.makedirs(os.path.join(PREPROCESS_DIR, 'train'), exist_ok=True)\n","    os.makedirs(os.path.join(PREPROCESS_DIR, 'val'), exist_ok=True)  # 검증 데이터용 디렉토리 추가\n","    os.makedirs(os.path.join(PREPROCESS_DIR, 'test'), exist_ok=True)\n","\n","def remove_background(image):\n","    # 기존 코드와 동일\n","    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","    masks = [\n","        cv2.inRange(hsv, np.array([0, 20, 20]), np.array([15, 255, 255])),   # red1\n","        cv2.inRange(hsv, np.array([165, 20, 20]), np.array([180, 255, 255])),# red2\n","        cv2.inRange(hsv, np.array([15, 20, 20]), np.array([40, 255, 255])),  # yellow\n","        cv2.inRange(hsv, np.array([5, 20, 20]), np.array([25, 255, 255]))    # orange\n","    ]\n","    color_mask = masks[0]\n","    for mask in masks[1:]:\n","        color_mask = cv2.bitwise_or(color_mask, mask)\n","\n","    edges = cv2.Canny(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), 100, 200)\n","    final_mask = cv2.bitwise_or(color_mask, edges)\n","\n","    kernel = np.ones((5, 5), np.uint8)\n","    final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_CLOSE, kernel)\n","    final_mask = cv2.dilate(final_mask, kernel, iterations=1)\n","\n","    return cv2.bitwise_and(image, image, mask=final_mask)\n","\n","def augment_with_noise(image):\n","    \"\"\"이미지에 랜덤 노이즈 추가\"\"\"\n","    noise = np.random.normal(0, random.uniform(5, 20), image.shape).astype(np.uint8)\n","    noisy_img = cv2.add(image, noise)\n","    return noisy_img\n","\n","def center_on_black(image):\n","    # 기존 코드와 동일\n","    black_bg = np.zeros((TARGET_SIZE, TARGET_SIZE, 3), dtype=np.uint8)\n","    h, w = image.shape[:2]\n","    scale = min(TARGET_SIZE / h, TARGET_SIZE / w) * 0.8\n","    new_h, new_w = int(h * scale), int(w * scale)\n","    resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)\n","    y_offset = (TARGET_SIZE - new_h) // 2\n","    x_offset = (TARGET_SIZE - new_w) // 2\n","    black_bg[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized\n","    return black_bg\n","\n","def basic_geometric_augment(image):\n","    \"\"\"샘플 수 맞추기용 기본 지오메트릭 변환\"\"\"\n","    h, w = image.shape[:2]\n","    center = (w // 2, h // 2)\n","\n","    transforms = [\n","        # 회전 (최소한의 각도)\n","        lambda img: cv2.warpAffine(img, cv2.getRotationMatrix2D(center, np.random.uniform(-10, 10), 1.0), (w, h)),\n","        # 스케일 (작은 변화)\n","        lambda img: cv2.resize(img, None, fx=np.random.uniform(0.9, 1.1), fy=np.random.uniform(0.9, 1.1)),\n","        # 밝기 (최소 조정)\n","        lambda img: cv2.convertScaleAbs(img, alpha=np.random.uniform(0.9, 1.1), beta=np.random.randint(-10, 10))\n","    ]\n","    return random.choice(transforms)(image)\n","\n","def split_train_val_files(input_dir):\n","    \"\"\"학습/검증 데이터 분할\"\"\"\n","    class_files = {}\n","    for filename in os.listdir(input_dir):\n","        if filename.endswith('.jpg'):\n","            class_name = filename.split('_')[0]\n","            if class_name not in class_files:\n","                class_files[class_name] = []\n","            class_files[class_name].append(filename)\n","\n","    train_files = []\n","    val_files = []\n","\n","    for class_name, files in class_files.items():\n","        files.sort()  # 파일 순서 보장\n","        train_class_files, val_class_files = train_test_split(\n","            files, test_size=0.2, random_state=42, shuffle=True\n","        )\n","        train_files.extend(train_class_files)\n","        val_files.extend(val_class_files)\n","\n","    return train_files, val_files\n","\n","def preprocess_and_select(files, split='train'):\n","    \"\"\"데이터 전처리 및 증강\"\"\"\n","    output_dir = os.path.join(PREPROCESS_DIR, split)\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # 클래스별 파일 분류\n","    class_files = {}\n","    for filename in files:\n","        class_name = filename.split('_')[0]\n","        if class_name not in class_files:\n","            class_files[class_name] = []\n","        class_files[class_name].append(filename)\n","\n","    # 최대 샘플 수 계산 (학습 데이터의 경우만)\n","    if split == 'train':\n","        max_samples = max(len(files) for files in class_files.values())\n","        print(f\"Maximum samples per class in {split}: {max_samples}\")\n","\n","    for class_name, class_files_list in class_files.items():\n","        class_dir = os.path.join(output_dir, class_name)\n","        os.makedirs(class_dir, exist_ok=True)\n","\n","        processed_images = []\n","        # 전처리 진행\n","        for filename in tqdm(class_files_list, desc=f\"Preprocessing {class_name} ({split})\", leave=False):\n","            image_path = os.path.join(TRAIN_DIR, filename)  # 원본 학습 데이터 경로 사용\n","            image = cv2.imread(image_path)\n","            if image is None:\n","                continue\n","            processed = remove_background(image)\n","            processed = center_on_black(processed)\n","            processed_images.append(processed)\n","\n","        # 학습 데이터이고 샘플 수가 부족한 경우 증강\n","        if split == 'train':\n","            current_count = len(processed_images)\n","            if current_count < max_samples:\n","                needed = max_samples - current_count\n","                for _ in range(needed):\n","                    base_img = random.choice(processed_images)\n","                    processed_images.append(basic_geometric_augment(base_img))\n","\n","        # 이미지 저장 (들여쓰기 수정)\n","        for i, img in enumerate(processed_images):\n","            save_name = f\"{class_name}_{i+1}.jpg\"\n","            cv2.imwrite(os.path.join(class_dir, save_name), img)\n","\n","def preprocess_test_data():\n","    \"\"\"테스트 데이터 전처리\"\"\"\n","    output_dir = os.path.join(PREPROCESS_DIR, 'test')\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    for filename in tqdm(os.listdir(TEST_DIR), desc=\"Processing test data\"):\n","        if not filename.endswith('.jpg'):\n","            continue\n","\n","        class_name = filename.split('_')[0]\n","        class_dir = os.path.join(output_dir, class_name)\n","        os.makedirs(class_dir, exist_ok=True)\n","\n","        image_path = os.path.join(TEST_DIR, filename)\n","        image = cv2.imread(image_path)\n","        if image is None:\n","            continue\n","\n","        processed = remove_background(image)\n","        processed = center_on_black(processed)\n","        cv2.imwrite(os.path.join(class_dir, filename), processed)\n","\n","def final_augment_image(image):\n","    \"\"\"최종 증강 기법\"\"\"\n","    h, w = image.shape[:2]\n","    center = (w // 2, h // 2)\n","\n","    # 기본 변환 (단일 변환, 강한 강도)\n","    basic_transforms = {\n","        'rotation': lambda img: cv2.warpAffine(img, cv2.getRotationMatrix2D(center, np.random.randint(-45, 45), 1.0), (w, h)),\n","        'brightness': lambda img: cv2.convertScaleAbs(img, alpha=np.random.uniform(0.6, 1.4), beta=np.random.randint(-50, 50)),\n","        'contrast': lambda img: cv2.convertScaleAbs(img, alpha=np.random.uniform(0.5, 1.5))\n","    }\n","\n","    # 복합 변환 (여러 변환 조합)\n","    composite_transforms = {\n","        'rotation_scale': lambda img: cv2.resize(\n","            cv2.warpAffine(img, cv2.getRotationMatrix2D(center, np.random.uniform(-30, 30), 1.0), (w, h)),\n","            None, fx=np.random.uniform(0.8, 1.3), fy=np.random.uniform(0.8, 1.3)\n","        ),\n","        #'brightness_blur': lambda img: cv2.GaussianBlur(\n","        #   cv2.convertScaleAbs(img, alpha=np.random.uniform(0.7, 1.3), beta=np.random.randint(-40, 40)),\n","        #   (5, 5), np.random.uniform(0.5, 2.0)\n","        #),\n","        'rotation_contrast': lambda img: cv2.convertScaleAbs(\n","            cv2.warpAffine(img, cv2.getRotationMatrix2D(center, np.random.uniform(-25, 25), 1.0), (w, h)),\n","            alpha=np.random.uniform(0.6, 1.4)\n","        )\n","    }\n","\n","    augmented_results = []\n","\n","    # 기본 변환 3개\n","    for key in basic_transforms.keys():\n","        aug_img = basic_transforms[key](image)\n","        augmented_results.append((f'basic_{key}', aug_img))\n","\n","    # 복합 변환 3개\n","    for key in composite_transforms.keys():\n","        aug_img = composite_transforms[key](image)\n","        augmented_results.append((f'composite_{key}', aug_img))\n","\n","    return augmented_results\n","\n","def final_augmentation(split='train'):\n","    \"\"\"최종 증강 (학습 데이터만)\"\"\"\n","    if split != 'train':\n","        return\n","\n","    train_dir = os.path.join(PREPROCESS_DIR, 'train')\n","    for class_name in os.listdir(train_dir):\n","        class_path = os.path.join(train_dir, class_name)\n","        if not os.path.isdir(class_path):\n","            continue\n","\n","        # 원본 이미지만 선택 (class_숫자.jpg 형태)\n","        images = [f for f in os.listdir(class_path) if f.endswith('.jpg') and len(f.split('_')) == 2]\n","        images.sort()\n","\n","        for filename in tqdm(images, desc=f\"Final Augmenting {class_name}\", leave=False):\n","            img_path = os.path.join(class_path, filename)\n","            img = cv2.imread(img_path)\n","            if img is None:\n","                continue\n","\n","            aug_imgs = final_augment_image(img)\n","            base_name = os.path.splitext(filename)[0]\n","\n","            for (transform_name, aimg) in aug_imgs:\n","                aug_path = os.path.join(class_path, f\"{base_name}_aug_{transform_name}.jpg\")\n","                cv2.imwrite(aug_path, aimg)\n","\n","def plot_examples_for_mixed():\n","    class_dir = os.path.join(PREPROCESS_DIR, 'train', 'mixed')\n","    if not os.path.exists(class_dir):\n","        print(\"No mixed class directory found.\")\n","        return\n","    images = [f for f in os.listdir(class_dir) if f.endswith('.jpg') and '_aug' not in f]\n","    if not images:\n","        print(\"No images found in mixed class.\")\n","        return\n","    img_path = os.path.join(class_dir, images[0])\n","    img = cv2.imread(img_path)\n","    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    aug_imgs = final_augment_image(img)\n","\n","    plt.figure(figsize=(15, 3))\n","    plt.subplot(1, len(aug_imgs) + 1, 1)\n","    plt.imshow(img_rgb)\n","    plt.title(\"Original\")\n","    plt.axis('off')\n","\n","    for i, (name, aimg) in enumerate(aug_imgs, 1):\n","        plt.subplot(1, len(aug_imgs) + 1, i+1)\n","        plt.imshow(cv2.cvtColor(aimg, cv2.COLOR_BGR2RGB))\n","        plt.title(name)  # 증강 기법 이름 표시\n","        plt.axis('off')\n","    plt.show()\n","\n","\n","if __name__ == \"__main__\":\n","    create_directories()\n","\n","    print(\"\\nSplitting training data into train/val...\")\n","    train_files, val_files = split_train_val_files(TRAIN_DIR)\n","\n","    print(\"\\nPreprocessing and augmenting training data...\")\n","    preprocess_and_select(train_files, split='train')\n","\n","    print(\"\\nPreprocessing validation data...\")\n","    preprocess_and_select(val_files, split='val')\n","\n","    print(\"\\nPreprocessing test data...\")\n","    preprocess_test_data()\n","\n","    print(\"\\nApplying final augmentation to training data...\")\n","    final_augmentation(split='train')\n","\n","    # 최종 데이터 수 출력\n","    for split in ['train', 'val', 'test']:\n","        split_dir = os.path.join(PREPROCESS_DIR, split)\n","        for class_name in os.listdir(split_dir):\n","            class_path = os.path.join(split_dir, class_name)\n","            if os.path.isdir(class_path):\n","                count = len([f for f in os.listdir(class_path) if f.endswith('.jpg')])\n","                print(f\"{split.capitalize()} - Class {class_name}: {count} images\")\n","    print(\"\\nShowing examples for mixed class final augmentation...\")\n","    plot_examples_for_mixed()"],"metadata":{"id":"FepzrDUl_ErE","executionInfo":{"status":"aborted","timestamp":1733657789603,"user_tz":-540,"elapsed":2,"user":{"displayName":"이성준","userId":"09916915111798806291"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 전처리 셀 2"],"metadata":{"id":"V-dy3uSNTgqw"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import cv2\n","import os\n","from tqdm import tqdm\n","import pickle\n","from sklearn.preprocessing import LabelEncoder\n","\n","# 경로 설정\n","PREPROCESS_DIR = '/tmp/preprocess_data'\n","FEATURE_DIR = '/tmp/feature_data'\n","os.makedirs(FEATURE_DIR, exist_ok=True)\n","\n","class FeatureExtractor:\n","    def __init__(self, input_size=256):\n","        self.input_size = input_size\n","        self.label_encoder = LabelEncoder()\n","\n","    def extract_color_features(self, img):\n","        \"\"\"컬러 관련 특징 추출\"\"\"\n","        # 각 채널별 평균과 표준편차\n","        means = img.mean(axis=(0, 1))\n","        stds = img.std(axis=(0, 1))\n","\n","        # 각 채널별 히스토그램\n","        features = []\n","        for i in range(3):  # RGB 각 채널\n","            hist = cv2.calcHist([img], [i], None, [32], [0, 256])\n","            hist = hist.flatten() / hist.sum()  # 정규화\n","            features.extend(hist)\n","\n","        features.extend(means)\n","        features.extend(stds)\n","        return np.array(features)\n","\n","    def extract_texture_features(self, img):\n","        \"\"\"텍스처 특징 추출\"\"\"\n","        # 그레이스케일 변환\n","        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","\n","        # Sobel 엣지 검출\n","        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n","        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n","\n","        # 엣지 강도와 방향\n","        magnitude = np.sqrt(sobelx**2 + sobely**2)\n","        direction = np.arctan2(sobely, sobelx)\n","\n","        # 통계적 특징\n","        features = [\n","            np.mean(magnitude),  # 평균 엣지 강도\n","            np.std(magnitude),   # 엣지 강도의 표준편차\n","            np.percentile(magnitude, 90),  # 90번째 퍼센타일\n","            np.mean(direction),  # 평균 엣지 방향\n","            np.std(direction),   # 엣지 방향의 표준편차\n","        ]\n","\n","        # LBP와 유사한 local 패턴 분석\n","        kernel_size = 3\n","        local_mean = cv2.blur(gray, (kernel_size, kernel_size))\n","        pattern = (gray > local_mean).astype(np.uint8)\n","        pattern_hist = cv2.calcHist([pattern], [0], None, [2], [0, 2])\n","        pattern_hist = pattern_hist.flatten() / pattern_hist.sum()\n","\n","        features.extend(pattern_hist)\n","        return np.array(features)\n","\n","    def extract_shape_features(self, img):\n","        \"\"\"형태 관련 특징 추출\"\"\"\n","        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","\n","        # 이진화 (이미 엣지 정보가 포함된 이미지이므로)\n","        _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n","\n","        # 컨투어 찾기\n","        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","        if not contours:\n","            return np.zeros(5)\n","\n","        # 가장 큰 컨투어 선택\n","        largest_contour = max(contours, key=cv2.contourArea)\n","\n","        # 특징 추출\n","        area = cv2.contourArea(largest_contour)\n","        perimeter = cv2.arcLength(largest_contour, True)\n","        hull = cv2.convexHull(largest_contour)\n","        hull_area = cv2.contourArea(hull)\n","\n","        features = [\n","            area / (self.input_size ** 2),  # 정규화된 면적\n","            perimeter / (4 * self.input_size),  # 정규화된 둘레\n","            hull_area / (self.input_size ** 2),  # 정규화된 컨벡스 헐 면적\n","            4 * np.pi * area / (perimeter ** 2) if perimeter > 0 else 0,  # 원형도\n","            area / hull_area if hull_area > 0 else 0  # 볼록도\n","        ]\n","\n","        return np.array(features)\n","\n","    def process_dataset(self, split='train'):\n","        \"\"\"데이터셋 처리 및 특징 추출\"\"\"\n","        data_dir = os.path.join(PREPROCESS_DIR, split)\n","        features_list = []\n","        labels = []\n","        image_paths = []\n","\n","        for class_name in os.listdir(data_dir):\n","            class_dir = os.path.join(data_dir, class_name)\n","            if not os.path.isdir(class_dir):\n","                continue\n","\n","            print(f\"Processing {split} - {class_name}\")\n","            for img_name in tqdm(os.listdir(class_dir)):\n","                if not img_name.endswith('.jpg'):\n","                    continue\n","\n","                img_path = os.path.join(class_dir, img_name)\n","                img = cv2.imread(img_path)\n","                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","                # 특징 추출\n","                color_features = self.extract_color_features(img)\n","                texture_features = self.extract_texture_features(img)\n","                shape_features = self.extract_shape_features(img)\n","\n","                # 모든 특징 합치기\n","                all_features = np.concatenate([\n","                    color_features,\n","                    texture_features,\n","                    shape_features\n","                ])\n","\n","                features_list.append(all_features)\n","                labels.append(class_name)\n","                image_paths.append(img_path)\n","\n","        # 특징을 numpy 배열로 변환\n","        X = np.array(features_list)\n","\n","        # 레이블 인코딩 (train일 때만 fit)\n","        if split == 'train':\n","            y = self.label_encoder.fit_transform(labels)\n","        else:\n","            y = self.label_encoder.transform(labels)\n","\n","        return X, y, image_paths\n","\n","    def prepare_data(self):\n","        \"\"\"전체 데이터셋 준비\"\"\"\n","        # 이미 분할된 데이터셋 처리\n","        print(\"Processing training data...\")\n","        X_train, y_train, train_paths = self.process_dataset('train')\n","\n","        print(\"Processing validation data...\")\n","        X_val, y_val, val_paths = self.process_dataset('val')\n","\n","        print(\"Processing test data...\")\n","        X_test, y_test, test_paths = self.process_dataset('test')\n","\n","        # 데이터 저장\n","        data = {\n","            'X_train': X_train,\n","            'X_val': X_val,\n","            'X_test': X_test,\n","            'y_train': y_train,\n","            'y_val': y_val,\n","            'y_test': y_test,\n","            'train_paths': train_paths,\n","            'val_paths': val_paths,\n","            'test_paths': test_paths,\n","            'label_encoder': self.label_encoder\n","        }\n","\n","        with open(os.path.join(FEATURE_DIR, 'multimodal_data.pkl'), 'wb') as f:\n","            pickle.dump(data, f)\n","\n","        print(\"\\nFeature shapes:\")\n","        print(f\"X_train: {X_train.shape}\")\n","        print(f\"X_val: {X_val.shape}\")\n","        print(f\"X_test: {X_test.shape}\")\n","\n","        print(\"\\nSample counts:\")\n","        print(f\"Training samples: {len(X_train)}\")\n","        print(f\"Validation samples: {len(X_val)}\")\n","        print(f\"Test samples: {len(X_test)}\")\n","\n","        return data\n","\n","# 특징 추출 실행\n","if __name__ == \"__main__\":\n","    extractor = FeatureExtractor()\n","    data = extractor.prepare_data()\n","    print(\"\\nFeature extraction completed and saved!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZaoVInP2qof","executionInfo":{"status":"ok","timestamp":1733588910857,"user_tz":-540,"elapsed":16739,"user":{"displayName":"쭈니스","userId":"12544842393218959388"}},"outputId":"883de46e-90b5-4f63-b034-294e42d30465"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing training data...\n","Processing train - banana\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 420/420 [00:03<00:00, 110.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing train - orange\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 420/420 [00:03<00:00, 110.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing train - mixed\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 420/420 [00:03<00:00, 109.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing train - apple\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 420/420 [00:03<00:00, 110.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing validation data...\n","Processing val - banana\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:00<00:00, 114.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing val - orange\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:00<00:00, 112.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing val - mixed\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:00<00:00, 113.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing val - apple\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:00<00:00, 111.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing test data...\n","Processing test - banana\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:00<00:00, 108.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing test - orange\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [00:00<00:00, 111.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing test - mixed\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:00<00:00, 113.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Processing test - apple\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19/19 [00:00<00:00, 111.98it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Feature shapes:\n","X_train: (1680, 114)\n","X_val: (49, 114)\n","X_test: (60, 114)\n","\n","Sample counts:\n","Training samples: 1680\n","Validation samples: 49\n","Test samples: 60\n","\n","Feature extraction completed and saved!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# 모델 구조 및 학습 셀"],"metadata":{"id":"V8396zFDTku4"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import pickle\n","import os\n","import cv2\n","import numpy as np\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from torchvision import transforms\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","class Config:\n","    IMAGE_SIZE = 256\n","    BATCH_SIZE = 8  # 배치 크기 감소\n","    NUM_WORKERS = 2\n","\n","    DROPOUT_RATE = 0.4  # 드롭아웃 증가\n","    HIDDEN_DIM = 256\n","    BN_MOMENTUM = 0.1  # BatchNorm 모멘텀 추가\n","\n","    LEARNING_RATE = 0.0001  # 학습률 감소\n","    NUM_EPOCHS = 150\n","    EARLY_STOP_PATIENCE = 10  # 얼리스탑 참을성 증가\n","    WEIGHT_DECAY = 0.001  # 정규화 강화\n","\n","    LABEL_SMOOTHING = 0.1\n","    GRAD_CLIP = 1.0  # Gradient Clipping 추가\n","\n","    WARMUP_EPOCHS = 5  # Warmup 추가\n","\n","    USE_SCHEDULER = True\n","    SCHEDULER_PATIENCE = 5\n","    SCHEDULER_FACTOR = 0.3\n","    MIN_LR = 1e-6\n","\n","    SPARSITY_WEIGHT = 0.0005\n","\n","    SAVE_DIR = 'model_checkpoints'\n","    os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","class MultiModalDataset(Dataset):\n","    def __init__(self, images_paths, features, labels, image_size=256):\n","        self.image_paths = images_paths\n","        self.features = torch.FloatTensor(features)\n","        self.labels = torch.LongTensor(labels)\n","        self.image_size = image_size\n","\n","        self.transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                              std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image = cv2.imread(self.image_paths[idx])\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = cv2.resize(image, (self.image_size, self.image_size))\n","        image = self.transform(image)\n","\n","        return {\n","            'image': image,\n","            'features': self.features[idx],\n","            'label': self.labels[idx]\n","        }\n","\n","class Expert(nn.Module):\n","    def __init__(self, feature_dim, config):\n","        super().__init__()\n","\n","        def conv_block(in_c, out_c):\n","            return nn.Sequential(\n","                nn.Conv2d(in_c, out_c, 3, padding=1),\n","                nn.BatchNorm2d(out_c, momentum=config.BN_MOMENTUM),  # 모멘텀 적용\n","                nn.ReLU(),\n","                nn.Conv2d(out_c, out_c, 3, padding=1),\n","                nn.BatchNorm2d(out_c, momentum=config.BN_MOMENTUM),  # 모멘텀 적용\n","                nn.ReLU(),\n","                nn.MaxPool2d(2),\n","                nn.Dropout2d(config.DROPOUT_RATE)\n","            )\n","\n","        self.image_encoder = nn.Sequential(\n","            conv_block(3, 64),\n","            conv_block(64, 128),\n","            conv_block(128, 256),\n","            conv_block(256, 512),\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","            nn.Flatten()\n","        )\n","\n","        # Feature encoder capacity 증가\n","        self.feature_encoder = nn.Sequential(\n","            nn.Linear(feature_dim, config.HIDDEN_DIM * 4),\n","            nn.BatchNorm1d(config.HIDDEN_DIM * 4, momentum=config.BN_MOMENTUM),\n","            nn.ReLU(),\n","            nn.Dropout(config.DROPOUT_RATE),\n","            nn.Linear(config.HIDDEN_DIM * 4, config.HIDDEN_DIM * 2),\n","            nn.BatchNorm1d(config.HIDDEN_DIM * 2, momentum=config.BN_MOMENTUM),\n","            nn.ReLU(),\n","            nn.Linear(config.HIDDEN_DIM * 2, config.HIDDEN_DIM),\n","            nn.BatchNorm1d(config.HIDDEN_DIM, momentum=config.BN_MOMENTUM),\n","            nn.ReLU()\n","        )\n","\n","        self.attention = nn.Sequential(\n","            nn.Linear(512 + config.HIDDEN_DIM, 256),\n","            nn.Tanh(),\n","            nn.Linear(256, 1),\n","            nn.Sigmoid()\n","        )\n","\n","        self.combined = nn.Sequential(\n","            nn.Linear(512 + config.HIDDEN_DIM, config.HIDDEN_DIM * 4),  # Capacity 증가\n","            nn.BatchNorm1d(config.HIDDEN_DIM * 4, momentum=config.BN_MOMENTUM),\n","            nn.ReLU(),\n","            nn.Dropout(config.DROPOUT_RATE),\n","            nn.Linear(config.HIDDEN_DIM * 4, config.HIDDEN_DIM * 2),\n","            nn.BatchNorm1d(config.HIDDEN_DIM * 2, momentum=config.BN_MOMENTUM),\n","            nn.ReLU(),\n","            nn.Linear(config.HIDDEN_DIM * 2, 1)\n","        )\n","\n","    def forward(self, images, features):\n","        img_feat = self.image_encoder(images)\n","        num_feat = self.feature_encoder(features)\n","\n","        combined = torch.cat([img_feat, num_feat], dim=1)\n","        attention_weights = self.attention(combined)\n","        attended_features = combined * attention_weights\n","\n","        score = self.combined(attended_features)\n","        return score\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=1, gamma=2, class_weights=None):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.class_weights = torch.tensor([1.0, 1.0, 3.0, 1.0]) if class_weights is None else class_weights\n","\n","    def forward(self, inputs, targets):\n","        ce_loss = F.cross_entropy(\n","            inputs, targets,\n","            weight=self.class_weights.to(inputs.device),\n","            label_smoothing=Config.LABEL_SMOOTHING,  # 레이블 스무딩 적용\n","            reduction='none'\n","        )\n","        pt = torch.exp(-ce_loss)\n","        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n","        return focal_loss.mean()\n","\n","class EarlyStopping:\n","    def __init__(self, patience=7, min_delta=0, path='best_model.pth'):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.path = path\n","        self.counter = 0\n","        self.best_loss = None\n","        self.early_stop = False\n","\n","    def __call__(self, val_loss, model):\n","        if self.best_loss is None:\n","            self.best_loss = val_loss\n","            self.save_checkpoint(model)\n","        elif val_loss > self.best_loss + self.min_delta:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_loss = val_loss\n","            self.save_checkpoint(model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, model):\n","        torch.save(model.state_dict(), self.path)\n","\n","class MoEMultiModal(nn.Module):\n","    def __init__(self, num_classes, feature_dim, config):\n","        super().__init__()\n","        self.num_classes = num_classes\n","\n","        self.experts = nn.ModuleList([\n","            Expert(feature_dim, config) for _ in range(num_classes)\n","        ])\n","\n","        self.gate = nn.Sequential(\n","            nn.Linear(feature_dim, config.HIDDEN_DIM * 2),  # Capacity 증가\n","            nn.BatchNorm1d(config.HIDDEN_DIM * 2, momentum=config.BN_MOMENTUM),\n","            nn.ReLU(),\n","            nn.Dropout(config.DROPOUT_RATE),\n","            nn.Linear(config.HIDDEN_DIM * 2, config.HIDDEN_DIM),\n","            nn.BatchNorm1d(config.HIDDEN_DIM, momentum=config.BN_MOMENTUM),\n","            nn.ReLU(),\n","            nn.Linear(config.HIDDEN_DIM, num_classes),\n","            nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self, images, features):\n","        gate_weights = self.gate(features)\n","\n","        expert_outputs = []\n","        for expert in self.experts:\n","            out = expert(images, features)\n","            expert_outputs.append(out)\n","\n","        expert_outputs = torch.cat(expert_outputs, dim=1)\n","        final_output = expert_outputs * gate_weights\n","\n","        return final_output\n","\n","def evaluate_one_epoch(model, data_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            images = batch['image'].to(device)\n","            features = batch['features'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            outputs = model(images, features)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    avg_loss = total_loss / len(data_loader)\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds, average='macro')\n","\n","    return avg_loss, accuracy, f1\n","\n","def get_lr_multiplier(epoch, warmup_epochs):\n","    if epoch < warmup_epochs:\n","        return (epoch + 1) / warmup_epochs\n","    return 1.0\n","\n","def train_moe_model(model, train_loader, val_loader, criterion, optimizer, config, device):\n","    early_stopping = EarlyStopping(\n","        patience=config.EARLY_STOP_PATIENCE,\n","        path=os.path.join(config.SAVE_DIR, 'best_moe_model.pth')\n","    )\n","\n","    if config.USE_SCHEDULER:\n","        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer, mode='min', patience=config.SCHEDULER_PATIENCE,\n","            factor=config.SCHEDULER_FACTOR, min_lr=config.MIN_LR, verbose=True\n","        )\n","\n","    train_losses, train_accs, train_f1s = [], [], []\n","    val_losses, val_accs, val_f1s = [], [], []\n","\n","    for epoch in range(config.NUM_EPOCHS):\n","        # Learning rate warmup\n","        if epoch < config.WARMUP_EPOCHS:\n","            lr_multiplier = get_lr_multiplier(epoch, config.WARMUP_EPOCHS)\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = config.LEARNING_RATE * lr_multiplier\n","\n","        # Training\n","        model.train()\n","        train_loss = 0.0\n","        train_preds = []\n","        train_labels = []\n","\n","        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.NUM_EPOCHS}'):\n","            images = batch['image'].to(device)\n","            features = batch['features'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images, features)\n","            loss = criterion(outputs, labels)\n","\n","            gate_sparsity = 0.0\n","            for expert in model.experts:\n","                gate_sparsity += torch.mean(torch.abs(expert.combined[-1].weight))\n","\n","            total_loss = loss + config.SPARSITY_WEIGHT * gate_sparsity\n","            total_loss.backward()\n","\n","            # Gradient Clipping\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRAD_CLIP)\n","\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            _, preds = torch.max(outputs, 1)\n","            train_preds.extend(preds.cpu().numpy())\n","            train_labels.extend(labels.cpu().numpy())\n","\n","        train_loss = train_loss / len(train_loader)\n","        train_acc = accuracy_score(train_labels, train_preds)\n","        train_f1 = f1_score(train_labels, train_preds, average='macro')\n","\n","        # Validation\n","        val_loss, val_acc, val_f1 = evaluate_one_epoch(model, val_loader, criterion, device)\n","\n","        # Save metrics\n","        train_losses.append(train_loss)\n","        train_accs.append(train_acc)\n","        train_f1s.append(train_f1)\n","        val_losses.append(val_loss)\n","        val_accs.append(val_acc)\n","        val_f1s.append(val_f1)\n","\n","        # Print progress\n","        print(f'\\nEpoch {epoch+1}/{config.NUM_EPOCHS}:')\n","        print(f'Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}')\n","        print(f'Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}')\n","\n","        if config.USE_SCHEDULER:\n","            scheduler.step(val_loss)\n","\n","        early_stopping(val_loss, model)\n","        if early_stopping.early_stop:\n","            print(\"\\nEarly stopping triggered\")\n","            break\n","\n","    # Plot training curves\n","    plot_metrics(train_losses, val_losses, 'Loss',\n","                os.path.join(config.SAVE_DIR, 'loss_curves.png'))\n","    plot_metrics(train_accs, val_accs, 'Accuracy',\n","                os.path.join(config.SAVE_DIR, 'accuracy_curves.png'))\n","    plot_metrics(train_f1s, val_f1s, 'F1 Score',\n","                os.path.join(config.SAVE_DIR, 'f1_curves.png'))\n","\n","def plot_confusion_matrix(true_labels, pred_labels, class_names, save_path):\n","    cm = confusion_matrix(true_labels, pred_labels)\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=class_names, yticklabels=class_names)\n","    plt.title('Confusion Matrix')\n","    plt.ylabel('True Label')\n","    plt.xlabel('Predicted Label')\n","    plt.tight_layout()\n","    plt.savefig(save_path)\n","    plt.close()\n","\n","def plot_metrics(train_metrics, val_metrics, metric_name, save_path):\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(train_metrics, label=f'Train {metric_name}')\n","    plt.plot(val_metrics, label=f'Val {metric_name}')\n","    plt.xlabel('Epoch')\n","    plt.ylabel(metric_name)\n","    plt.legend()\n","    plt.title(f'{metric_name} over Training')\n","    plt.tight_layout()\n","    plt.savefig(save_path)\n","    plt.close()\n","\n","def calculate_class_weights(y_train):\n","    class_counts = np.bincount(y_train)\n","    total = len(y_train)\n","    weights = total / (len(class_counts) * class_counts)\n","    return torch.FloatTensor(weights)\n","\n","def main():\n","    torch.cuda.empty_cache()\n","    config = Config()\n","\n","    # 데이터 로드\n","    with open(os.path.join(FEATURE_DIR, 'multimodal_data.pkl'), 'rb') as f:\n","        data = pickle.load(f)\n","\n","    # 클래스 가중치 계산\n","    class_weights = calculate_class_weights(data['y_train'])\n","\n","    # Dataset and DataLoader setup\n","    train_dataset = MultiModalDataset(\n","        data['train_paths'], data['X_train'], data['y_train'],\n","        image_size=config.IMAGE_SIZE\n","    )\n","    val_dataset = MultiModalDataset(\n","        data['val_paths'], data['X_val'], data['y_val'],\n","        image_size=config.IMAGE_SIZE\n","    )\n","    test_dataset = MultiModalDataset(\n","        data['test_paths'], data['X_test'], data['y_test'],\n","        image_size=config.IMAGE_SIZE\n","    )\n","\n","    train_loader = DataLoader(\n","        train_dataset, batch_size=config.BATCH_SIZE,\n","        shuffle=True, num_workers=config.NUM_WORKERS\n","    )\n","    val_loader = DataLoader(\n","        val_dataset, batch_size=config.BATCH_SIZE,\n","        shuffle=False, num_workers=config.NUM_WORKERS\n","    )\n","    test_loader = DataLoader(\n","        test_dataset, batch_size=config.BATCH_SIZE,\n","        shuffle=False, num_workers=config.NUM_WORKERS\n","    )\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = MoEMultiModal(\n","        num_classes=len(data['label_encoder'].classes_),\n","        feature_dim=data['X_train'].shape[1],\n","        config=config\n","    ).to(device)\n","\n","    criterion = FocalLoss(gamma=2, class_weights=class_weights)\n","    optimizer = torch.optim.AdamW(\n","        model.parameters(),\n","        lr=config.LEARNING_RATE,\n","        weight_decay=config.WEIGHT_DECAY\n","    )\n","\n","    # Training\n","    print(\"Starting training...\")\n","    train_moe_model(\n","        model=model,\n","        train_loader=train_loader,\n","        val_loader=val_loader,\n","        criterion=criterion,\n","        optimizer=optimizer,\n","        config=config,\n","        device=device\n","    )\n","\n","    # Final evaluation\n","    print(\"\\nLoading best model for final evaluation...\")\n","    model.load_state_dict(torch.load(os.path.join(config.SAVE_DIR, 'best_moe_model.pth')))\n","\n","    print(\"\\nValidation Set Performance:\")\n","    val_loss, val_acc, val_f1 = evaluate_one_epoch(model, val_loader, criterion, device)\n","    print(f\"Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, F1 Score: {val_f1:.4f}\")\n","\n","    print(\"\\nTest Set Performance:\")\n","    test_loss, test_acc, test_f1 = evaluate_one_epoch(model, test_loader, criterion, device)\n","    print(f\"Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 Score: {test_f1:.4f}\")\n","\n","    # Detailed classification report for test set\n","    all_preds = []\n","    all_labels = []\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            images = batch['image'].to(device)\n","            features = batch['features'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            outputs = model(images, features)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    print(\"\\nDetailed Classification Report:\")\n","    print(classification_report(all_labels, all_preds,\n","                              target_names=data['label_encoder'].classes_))\n","\n","    plot_confusion_matrix(\n","        all_labels, all_preds,\n","        data['label_encoder'].classes_,\n","        os.path.join(config.SAVE_DIR, 'confusion_matrix.png')\n","    )\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NMs2y_ceOssD","executionInfo":{"status":"ok","timestamp":1733592329359,"user_tz":-540,"elapsed":716789,"user":{"displayName":"쭈니스","userId":"12544842393218959388"}},"outputId":"589e8428-fd38-437e-aaf2-b9f6d507359b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training...\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/150: 100%|██████████| 210/210 [00:20<00:00, 10.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/150:\n","Train - Loss: 0.7316, Acc: 0.3881, F1: 0.3835\n","Val   - Loss: 0.6823, Acc: 0.4694, F1: 0.3588\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/150: 100%|██████████| 210/210 [00:20<00:00, 10.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2/150:\n","Train - Loss: 0.5544, Acc: 0.5893, F1: 0.5759\n","Val   - Loss: 0.3361, Acc: 0.7347, F1: 0.6491\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/150: 100%|██████████| 210/210 [00:19<00:00, 10.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 3/150:\n","Train - Loss: 0.4155, Acc: 0.6976, F1: 0.6954\n","Val   - Loss: 0.2464, Acc: 0.8367, F1: 0.7309\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/150: 100%|██████████| 210/210 [00:19<00:00, 10.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4/150:\n","Train - Loss: 0.3729, Acc: 0.7262, F1: 0.7245\n","Val   - Loss: 0.1736, Acc: 0.8776, F1: 0.7702\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/150: 100%|██████████| 210/210 [00:19<00:00, 10.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 5/150:\n","Train - Loss: 0.3302, Acc: 0.7732, F1: 0.7727\n","Val   - Loss: 0.1673, Acc: 0.9184, F1: 0.8120\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/150: 100%|██████████| 210/210 [00:19<00:00, 10.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 6/150:\n","Train - Loss: 0.3058, Acc: 0.7935, F1: 0.7925\n","Val   - Loss: 0.1598, Acc: 0.8980, F1: 0.7970\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/150: 100%|██████████| 210/210 [00:19<00:00, 10.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 7/150:\n","Train - Loss: 0.3060, Acc: 0.8071, F1: 0.8073\n","Val   - Loss: 0.1393, Acc: 0.9184, F1: 0.8520\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/150: 100%|██████████| 210/210 [00:19<00:00, 10.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 8/150:\n","Train - Loss: 0.2857, Acc: 0.8202, F1: 0.8200\n","Val   - Loss: 0.1527, Acc: 0.9184, F1: 0.8890\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/150: 100%|██████████| 210/210 [00:19<00:00, 10.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 9/150:\n","Train - Loss: 0.2471, Acc: 0.8351, F1: 0.8350\n","Val   - Loss: 0.1643, Acc: 0.8776, F1: 0.8106\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/150: 100%|██████████| 210/210 [00:19<00:00, 10.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 10/150:\n","Train - Loss: 0.2595, Acc: 0.8488, F1: 0.8488\n","Val   - Loss: 0.1475, Acc: 0.8980, F1: 0.8249\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/150: 100%|██████████| 210/210 [00:19<00:00, 10.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 11/150:\n","Train - Loss: 0.2334, Acc: 0.8506, F1: 0.8505\n","Val   - Loss: 0.1155, Acc: 0.9388, F1: 0.9042\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/150: 100%|██████████| 210/210 [00:19<00:00, 10.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 12/150:\n","Train - Loss: 0.2375, Acc: 0.8583, F1: 0.8584\n","Val   - Loss: 0.1668, Acc: 0.9184, F1: 0.8682\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/150: 100%|██████████| 210/210 [00:19<00:00, 10.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 13/150:\n","Train - Loss: 0.2272, Acc: 0.8619, F1: 0.8616\n","Val   - Loss: 0.1156, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/150: 100%|██████████| 210/210 [00:19<00:00, 10.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 14/150:\n","Train - Loss: 0.2200, Acc: 0.8726, F1: 0.8727\n","Val   - Loss: 0.1378, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/150: 100%|██████████| 210/210 [00:19<00:00, 10.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 15/150:\n","Train - Loss: 0.2040, Acc: 0.8964, F1: 0.8965\n","Val   - Loss: 0.1134, Acc: 0.9184, F1: 0.8520\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16/150: 100%|██████████| 210/210 [00:19<00:00, 10.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 16/150:\n","Train - Loss: 0.1978, Acc: 0.8881, F1: 0.8882\n","Val   - Loss: 0.1412, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17/150: 100%|██████████| 210/210 [00:19<00:00, 10.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 17/150:\n","Train - Loss: 0.2084, Acc: 0.8917, F1: 0.8916\n","Val   - Loss: 0.1200, Acc: 0.9184, F1: 0.8107\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18/150: 100%|██████████| 210/210 [00:19<00:00, 10.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 18/150:\n","Train - Loss: 0.1756, Acc: 0.9030, F1: 0.9030\n","Val   - Loss: 0.1137, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19/150: 100%|██████████| 210/210 [00:20<00:00, 10.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 19/150:\n","Train - Loss: 0.2037, Acc: 0.8964, F1: 0.8960\n","Val   - Loss: 0.1387, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 20/150: 100%|██████████| 210/210 [00:20<00:00, 10.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 20/150:\n","Train - Loss: 0.1745, Acc: 0.9125, F1: 0.9126\n","Val   - Loss: 0.1045, Acc: 0.9592, F1: 0.9396\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 21/150: 100%|██████████| 210/210 [00:20<00:00, 10.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 21/150:\n","Train - Loss: 0.1719, Acc: 0.9119, F1: 0.9120\n","Val   - Loss: 0.1466, Acc: 0.9184, F1: 0.8520\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 22/150: 100%|██████████| 210/210 [00:19<00:00, 10.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 22/150:\n","Train - Loss: 0.1857, Acc: 0.9119, F1: 0.9121\n","Val   - Loss: 0.1298, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 23/150: 100%|██████████| 210/210 [00:20<00:00, 10.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 23/150:\n","Train - Loss: 0.1590, Acc: 0.9196, F1: 0.9199\n","Val   - Loss: 0.1228, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 24/150: 100%|██████████| 210/210 [00:19<00:00, 10.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 24/150:\n","Train - Loss: 0.1630, Acc: 0.9220, F1: 0.9219\n","Val   - Loss: 0.1264, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 25/150: 100%|██████████| 210/210 [00:19<00:00, 10.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 25/150:\n","Train - Loss: 0.1444, Acc: 0.9262, F1: 0.9262\n","Val   - Loss: 0.0932, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 26/150: 100%|██████████| 210/210 [00:19<00:00, 10.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 26/150:\n","Train - Loss: 0.1642, Acc: 0.9179, F1: 0.9179\n","Val   - Loss: 0.1136, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 27/150: 100%|██████████| 210/210 [00:19<00:00, 10.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 27/150:\n","Train - Loss: 0.1493, Acc: 0.9274, F1: 0.9273\n","Val   - Loss: 0.1471, Acc: 0.9184, F1: 0.8102\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 28/150: 100%|██████████| 210/210 [00:19<00:00, 10.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 28/150:\n","Train - Loss: 0.1391, Acc: 0.9375, F1: 0.9376\n","Val   - Loss: 0.1760, Acc: 0.9184, F1: 0.8520\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 29/150: 100%|██████████| 210/210 [00:19<00:00, 10.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 29/150:\n","Train - Loss: 0.1536, Acc: 0.9280, F1: 0.9280\n","Val   - Loss: 0.1086, Acc: 0.9592, F1: 0.9005\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 30/150: 100%|██████████| 210/210 [00:19<00:00, 10.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 30/150:\n","Train - Loss: 0.1422, Acc: 0.9333, F1: 0.9333\n","Val   - Loss: 0.1334, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 31/150: 100%|██████████| 210/210 [00:19<00:00, 10.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 31/150:\n","Train - Loss: 0.1895, Acc: 0.9268, F1: 0.9271\n","Val   - Loss: 0.1454, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 32/150: 100%|██████████| 210/210 [00:20<00:00, 10.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 32/150:\n","Train - Loss: 0.1381, Acc: 0.9381, F1: 0.9383\n","Val   - Loss: 0.1402, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 33/150: 100%|██████████| 210/210 [00:19<00:00, 10.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 33/150:\n","Train - Loss: 0.1244, Acc: 0.9488, F1: 0.9490\n","Val   - Loss: 0.1341, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 34/150: 100%|██████████| 210/210 [00:19<00:00, 10.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 34/150:\n","Train - Loss: 0.1279, Acc: 0.9470, F1: 0.9471\n","Val   - Loss: 0.1235, Acc: 0.9388, F1: 0.8844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 35/150: 100%|██████████| 210/210 [00:19<00:00, 10.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 35/150:\n","Train - Loss: 0.1264, Acc: 0.9446, F1: 0.9447\n","Val   - Loss: 0.1247, Acc: 0.9388, F1: 0.8844\n","\n","Early stopping triggered\n","\n","Loading best model for final evaluation...\n","\n","Validation Set Performance:\n","Loss: 0.0932, Accuracy: 0.9388, F1 Score: 0.8844\n","\n","Test Set Performance:\n","Loss: 0.1822, Accuracy: 0.9000, F1 Score: 0.8216\n","\n","Detailed Classification Report:\n","              precision    recall  f1-score   support\n","\n","       apple       0.90      0.95      0.92        19\n","      banana       0.89      0.94      0.92        18\n","       mixed       0.67      0.40      0.50         5\n","      orange       0.94      0.94      0.94        18\n","\n","    accuracy                           0.90        60\n","   macro avg       0.85      0.81      0.82        60\n","weighted avg       0.89      0.90      0.89        60\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"8qNVQGLSTPPA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 찐 다시"],"metadata":{"id":"Vkn5nBTM7mtH"}},{"cell_type":"code","source":["import os, cv2, numpy as np\n","from pathlib import Path\n","import shutil\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import random\n","\n","# 경로 설정\n","TRAIN_DIR = '/tmp/train_zip/train'\n","TEST_DIR = '/tmp/test_zip/test'\n","PREPROCESS_DIR = '/tmp/preprocessed_data'\n","TARGET_SIZE = 256\n","\n","random.seed(42)  # 재현성을 위한 시드 설정\n","\n","def create_directories():\n","    \"\"\"작업 디렉토리 생성\"\"\"\n","    if os.path.exists(PREPROCESS_DIR):\n","        shutil.rmtree(PREPROCESS_DIR)\n","    os.makedirs(os.path.join(PREPROCESS_DIR, 'train'), exist_ok=True)\n","    os.makedirs(os.path.join(PREPROCESS_DIR, 'validation'), exist_ok=True)\n","    os.makedirs(os.path.join(PREPROCESS_DIR, 'test'), exist_ok=True)\n","\n","def remove_background(image):\n","    \"\"\"과일 이미지에서 배경 제거\"\"\"\n","    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","\n","    # HSV 색상 범위 정의\n","    masks = [\n","        cv2.inRange(hsv, np.array([0, 20, 20]), np.array([15, 255, 255])),    # red1\n","        cv2.inRange(hsv, np.array([165, 20, 20]), np.array([180, 255, 255])), # red2\n","        cv2.inRange(hsv, np.array([15, 20, 20]), np.array([40, 255, 255])),   # yellow-orange\n","        cv2.inRange(hsv, np.array([40, 20, 20]), np.array([80, 255, 255])),   # green\n","        cv2.inRange(hsv, np.array([100, 20, 20]), np.array([140, 255, 255]))  # blue\n","    ]\n","\n","    # 모든 마스크 결합\n","    color_mask = masks[0]\n","    for mask in masks[1:]:\n","        color_mask = cv2.bitwise_or(color_mask, mask)\n","\n","    # 엣지 검출 및 마스크와 결합\n","    edges = cv2.Canny(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), 100, 200)\n","    final_mask = cv2.bitwise_or(color_mask, edges)\n","\n","    # 마스크 정제\n","    kernel = np.ones((5,5), np.uint8)\n","    final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_CLOSE, kernel)\n","    final_mask = cv2.dilate(final_mask, kernel, iterations=1)\n","\n","    return cv2.bitwise_and(image, image, mask=final_mask)\n","\n","def center_on_black(image):\n","    \"\"\"이미지를 검은 배경 중앙에 위치시키기\"\"\"\n","    black_bg = np.zeros((TARGET_SIZE, TARGET_SIZE, 3), dtype=np.uint8)\n","    h, w = image.shape[:2]\n","    scale = min(TARGET_SIZE / h, TARGET_SIZE / w) * 0.8\n","    new_h, new_w = int(h * scale), int(w * scale)\n","    resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)\n","\n","    y_offset = (TARGET_SIZE - new_h) // 2\n","    x_offset = (TARGET_SIZE - new_w) // 2\n","    black_bg[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized\n","\n","    return black_bg\n","\n","def basic_transforms(image):\n","    \"\"\"기본 증강: 회전과 대칭만 사용\"\"\"\n","    h, w = image.shape[:2]\n","    center = (w // 2, h // 2)\n","\n","    # 변환 종류 선택 (회전 또는 대칭)\n","    transform_type = random.choice(['flip', 'rotation'])\n","\n","    if transform_type == 'flip':\n","        # 좌우 대칭만 적용\n","        return cv2.flip(image.copy(), 1)\n","    else:\n","        # 90도 단위 회전\n","        angle = random.choice([90, 180, 270])\n","        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n","        return cv2.warpAffine(image.copy(), M, (w, h),\n","                            flags=cv2.INTER_LINEAR,\n","                            borderMode=cv2.BORDER_CONSTANT)\n","\n","def advanced_transforms(image):\n","   \"\"\"고급 증강: 기하 변환 위주로 적용\"\"\"\n","   h, w = image.shape[:2]\n","   center = (w // 2, h // 2)\n","\n","   def affine_transform(img):\n","       \"\"\"어파인 변환\"\"\"\n","       src_pts = np.float32([[0,0], [w-1,0], [0,h-1]])\n","       dst_pts = src_pts + np.random.uniform(-w*0.05, w*0.05, src_pts.shape)\n","       M = cv2.getAffineTransform(src_pts, dst_pts)\n","       return cv2.warpAffine(img, M, (w,h), borderMode=cv2.BORDER_CONSTANT)\n","\n","   def scale_variation(img):\n","       \"\"\"크기 변화\"\"\"\n","       scale = np.random.uniform(0.9, 1.1)\n","       new_size = (int(w * scale), int(h * scale))\n","       scaled = cv2.resize(img, new_size)\n","\n","       result = np.zeros_like(img)\n","       y_offset = (h - scaled.shape[0]) // 2\n","       x_offset = (w - scaled.shape[1]) // 2\n","\n","       y_start = max(0, y_offset)\n","       y_end = min(h, y_offset + scaled.shape[0])\n","       x_start = max(0, x_offset)\n","       x_end = min(w, x_offset + scaled.shape[1])\n","\n","       result[y_start:y_end, x_start:x_end] = scaled[:y_end-y_start, :x_end-x_start]\n","       return result\n","\n","   def perspective_transform(img):\n","       \"\"\"원근 변환\"\"\"\n","       src_pts = np.float32([[0,0], [w-1,0], [0,h-1], [w-1,h-1]])\n","       dst_pts = src_pts + np.random.uniform(-w*0.05, w*0.05, src_pts.shape)\n","       M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n","       return cv2.warpPerspective(img, M, (w,h), borderMode=cv2.BORDER_CONSTANT)\n","\n","   def shear_transform(img):\n","       \"\"\"전단 변환\"\"\"\n","       shear_factor = np.random.uniform(-0.1, 0.1)\n","       M = np.float32([[1, shear_factor, 0], [0, 1, 0]])\n","       return cv2.warpAffine(img, M, (w,h), borderMode=cv2.BORDER_CONSTANT)\n","\n","   def elastic_transform(img):\n","       \"\"\"탄성 변형\"\"\"\n","       dx = np.random.uniform(-5, 5, (h,w))\n","       dy = np.random.uniform(-5, 5, (h,w))\n","       x, y = np.meshgrid(np.arange(w), np.arange(h))\n","       map_x = np.float32(x + dx)\n","       map_y = np.float32(y + dy)\n","       return cv2.remap(img, map_x, map_y, cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n","\n","   transforms = {\n","       'affine': affine_transform,\n","       'scale': scale_variation,\n","       'perspective': perspective_transform,\n","       'shear': shear_transform,\n","       'elastic': elastic_transform\n","   }\n","\n","   # 3개의 랜덤한 기하 변환 적용\n","   chosen_transforms = random.sample(list(transforms.keys()), k=3)\n","   result_image = image.copy()\n","\n","   for transform_name in chosen_transforms:\n","       result_image = transforms[transform_name](result_image)\n","\n","   return result_image\n","\n","def preprocess_and_split():\n","   \"\"\"데이터 전처리 및 분할\"\"\"\n","   # 클래스별 파일 정리\n","   class_files = {}\n","   for filename in os.listdir(TRAIN_DIR):\n","       if filename.endswith('.jpg'):\n","           class_name = filename.split('_')[0]\n","           if class_name not in class_files:\n","               class_files[class_name] = []\n","           class_files[class_name].append(filename)\n","\n","   # 8:2 분할\n","   train_files = {}\n","   val_files = {}\n","   for class_name, files in class_files.items():\n","       random.shuffle(files)\n","       split_idx = int(len(files) * 0.8)\n","       train_files[class_name] = files[:split_idx]\n","       val_files[class_name] = files[split_idx:]\n","\n","   # 학습 세트 최대 샘플 수 확인\n","   train_max_samples = max(len(files) for files in train_files.values())\n","   print(f\"Maximum samples per class in training set: {train_max_samples}\")\n","\n","   # 1단계: 학습 세트 처리\n","   for class_name, files in train_files.items():\n","       print(f\"\\nProcessing {class_name}...\")\n","       processed_images = []\n","       processed_backgrounds = []\n","\n","       # 원본 이미지 로드\n","       for filename in tqdm(files, desc=\"Loading original images\"):\n","           image = cv2.imread(os.path.join(TRAIN_DIR, filename))\n","           if image is None:\n","               continue\n","           processed_images.append(image)\n","\n","           # 배경 제거 버전도 저장\n","           bg_removed = remove_background(image.copy())\n","           centered = center_on_black(bg_removed)\n","           processed_backgrounds.append(centered)\n","\n","       # 기본 증강으로 샘플 수 맞추기\n","       if len(processed_images) < train_max_samples:\n","           needed = train_max_samples - len(processed_images)\n","           original_images = processed_images.copy()\n","\n","           for _ in tqdm(range(needed), desc=\"Basic augmentation\"):\n","               idx = random.randint(0, len(original_images)-1)\n","               # 원본에 기본 증강 적용\n","               aug_img = basic_transforms(original_images[idx])\n","               # 증강된 이미지의 배경 제거 버전 생성\n","               aug_bg_removed = remove_background(aug_img)\n","               aug_centered = center_on_black(aug_bg_removed)\n","\n","               processed_images.append(aug_img)\n","               processed_backgrounds.append(aug_centered)\n","\n","       # 2단계: 고급 증강 (배경이 제거된 이미지에 적용)\n","       augmented_images = []\n","       for idx in tqdm(range(len(processed_backgrounds)), desc=\"Advanced augmentation\"):\n","           base_img = processed_backgrounds[idx]\n","           # 각 이미지당 6개의 증강\n","           for _ in range(6):\n","               aug_img = advanced_transforms(base_img.copy())\n","               augmented_images.append(aug_img)\n","# 저장\n","       save_dir = os.path.join(PREPROCESS_DIR, 'train', class_name)\n","       os.makedirs(save_dir, exist_ok=True)\n","\n","       # 기본 증강 이미지 저장 (배경 제거 버전)\n","       for i, img in enumerate(processed_backgrounds):\n","           save_name = f\"{class_name}_orig_{i+1}.jpg\"\n","           cv2.imwrite(os.path.join(save_dir, save_name), img)\n","\n","       # 고급 증강 이미지 저장\n","       for i, img in enumerate(augmented_images):\n","           save_name = f\"{class_name}_adv_{i+1}.jpg\"\n","           cv2.imwrite(os.path.join(save_dir, save_name), img)\n","\n","       print(f\"Final count for {class_name}: {len(processed_backgrounds) + len(augmented_images)}\")\n","\n","   # Validation 세트 처리\n","   for class_name, files in val_files.items():\n","       save_dir = os.path.join(PREPROCESS_DIR, 'validation', class_name)\n","       os.makedirs(save_dir, exist_ok=True)\n","\n","       for filename in tqdm(files, desc=f\"Processing validation {class_name}\"):\n","           image = cv2.imread(os.path.join(TRAIN_DIR, filename))\n","           if image is None:\n","               continue\n","           processed = remove_background(image)\n","           processed = center_on_black(processed)\n","           save_name = f\"{class_name}_{filename}\"\n","           cv2.imwrite(os.path.join(save_dir, save_name), processed)\n","\n","def process_test_images():\n","   \"\"\"테스트 세트 처리\"\"\"\n","   for filename in tqdm(os.listdir(TEST_DIR), desc=\"Processing test images\"):\n","       if not filename.endswith('.jpg'):\n","           continue\n","\n","       class_name = filename.split('_')[0]\n","       save_dir = os.path.join(PREPROCESS_DIR, 'test', class_name)\n","       os.makedirs(save_dir, exist_ok=True)\n","\n","       image = cv2.imread(os.path.join(TEST_DIR, filename))\n","       if image is None:\n","           continue\n","\n","       processed = remove_background(image)\n","       processed = center_on_black(processed)\n","       cv2.imwrite(os.path.join(save_dir, filename), processed)\n","\n","def visualize_augmentations():\n","   \"\"\"증강 결과 시각화\"\"\"\n","   train_dir = os.path.join(PREPROCESS_DIR, 'train')\n","   plt.figure(figsize=(20, 10))\n","\n","   # 원본 이미지 선택\n","   sample_class = random.choice(os.listdir(TRAIN_DIR))\n","   image_path = os.path.join(TRAIN_DIR, random.choice([f for f in os.listdir(os.path.join(TRAIN_DIR, sample_class))\n","                                                      if f.endswith('.jpg')]))\n","   original_img = cv2.imread(image_path)\n","\n","   # 원본 이미지 배경 제거 및 중앙 정렬\n","   bg_removed = remove_background(original_img.copy())\n","   centered = center_on_black(bg_removed)\n","\n","   # 첫 번째 줄: 원본과 기본 증강\n","   plt.subplot(2, 5, 1)\n","   plt.imshow(cv2.cvtColor(centered, cv2.COLOR_BGR2RGB))\n","   plt.title(\"Original\")\n","   plt.axis('off')\n","\n","   # 기본 증강 4개 표시\n","   for i in range(4):\n","       plt.subplot(2, 5, i+2)\n","       # 원본에 기본 증강 적용\n","       augmented = basic_transforms(original_img.copy())\n","       # 증강된 이미지 배경 제거 및 중앙 정렬\n","       aug_bg_removed = remove_background(augmented)\n","       aug_centered = center_on_black(aug_bg_removed)\n","       plt.imshow(cv2.cvtColor(aug_centered, cv2.COLOR_BGR2RGB))\n","       plt.title(f\"Basic Aug {i+1}\")\n","       plt.axis('off')\n","\n","   # 두 번째 줄: 고급 증강\n","   for i in range(5):\n","       plt.subplot(2, 5, i+6)\n","       # 배경이 제거된 이미지에 고급 증강 적용\n","       augmented = advanced_transforms(centered.copy())\n","       plt.imshow(cv2.cvtColor(augmented, cv2.COLOR_BGR2RGB))\n","       plt.title(f\"Advanced Aug {i+1}\")\n","       plt.axis('off')\n","\n","   plt.tight_layout()\n","   plt.show()\n","\n","if __name__ == \"__main__\":\n","   print(\"Creating directories...\")\n","   create_directories()\n","\n","   print(\"\\nPreprocessing and splitting training data...\")\n","   preprocess_and_split()\n","\n","   print(\"\\nProcessing test data...\")\n","   process_test_images()\n","\n","   print(\"\\nShowing augmentation examples...\")\n","   visualize_augmentations()\n","\n","   print(\"Done!\")"],"metadata":{"id":"FBL-Pg411NY-","colab":{"base_uri":"https://localhost:8080/","height":597},"executionInfo":{"status":"error","timestamp":1733572264774,"user_tz":-540,"elapsed":2919,"user":{"displayName":"쭈니스","userId":"12544842393218959388"}},"outputId":"3f41d4d9-da4a-4eda-c2c1-7431c57fb1aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating directories...\n","\n","Preprocessing and splitting training data...\n","Maximum samples per class in training set: 60\n","\n","Processing banana...\n"]},{"output_type":"stream","name":"stderr","text":["Loading original images: 100%|██████████| 58/58 [00:01<00:00, 37.64it/s]\n","Basic augmentation: 100%|██████████| 2/2 [00:00<00:00, 74.01it/s]\n","Advanced augmentation:   0%|          | 0/60 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"error","evalue":"OpenCV(4.10.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:3624: error: (-215:Assertion failed) src.checkVector(2, CV_32F) == 4 && dst.checkVector(2, CV_32F) == 4 in function 'getPerspectiveTransform'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-ebf564741f6e>\u001b[0m in \u001b[0;36m<cell line: 314>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPreprocessing and splitting training data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m    \u001b[0mpreprocess_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nProcessing test data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-ebf564741f6e>\u001b[0m in \u001b[0;36mpreprocess_and_split\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m            \u001b[0;31m# 각 이미지당 6개의 증강\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                \u001b[0maug_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madvanced_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                \u001b[0maugmented_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;31m# 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-ebf564741f6e>\u001b[0m in \u001b[0;36madvanced_transforms\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mtransform_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchosen_transforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m        \u001b[0mresult_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransform_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mresult_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-ebf564741f6e>\u001b[0m in \u001b[0;36mperspective_transform\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    118\u001b[0m        \u001b[0msrc_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m        \u001b[0mdst_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_pts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_pts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m        \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPerspectiveTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_pts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_pts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarpPerspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mborderMode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBORDER_CONSTANT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:3624: error: (-215:Assertion failed) src.checkVector(2, CV_32F) == 4 && dst.checkVector(2, CV_32F) == 4 in function 'getPerspectiveTransform'\n"]}]},{"cell_type":"code","source":["import os, cv2, numpy as np\n","from pathlib import Path\n","import shutil\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import random\n","\n","# 경로 설정\n","TRAIN_DIR = '/tmp/train_zip/train'\n","TEST_DIR = '/tmp/test_zip/test'\n","PREPROCESS_DIR = '/tmp/preprocess_data'\n","TARGET_SIZE = 256\n","\n","random.seed(42)  # 재현성 있는 랜덤\n","\n","def create_directories():\n","    if os.path.exists(PREPROCESS_DIR):\n","        shutil.rmtree(PREPROCESS_DIR)\n","    os.makedirs(os.path.join(PREPROCESS_DIR, 'train'), exist_ok=True)\n","    os.makedirs(os.path.join(PREPROCESS_DIR, 'test'), exist_ok=True)\n","\n","def remove_background(image):\n","    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","    # 컬러 마스크 범위 (느슨하게)\n","    masks = [\n","        cv2.inRange(hsv, np.array([0, 20, 20]), np.array([15, 255, 255])),   # red1\n","        cv2.inRange(hsv, np.array([165, 20, 20]), np.array([180, 255, 255])),# red2\n","        cv2.inRange(hsv, np.array([15, 20, 20]), np.array([40, 255, 255])),  # yellow\n","        cv2.inRange(hsv, np.array([5, 20, 20]), np.array([25, 255, 255]))    # orange\n","    ]\n","    color_mask = masks[0]\n","    for mask in masks[1:]:\n","        color_mask = cv2.bitwise_or(color_mask, mask)\n","\n","    # 엄격한 엣지 검출\n","    edges = cv2.Canny(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), 100, 200)\n","    final_mask = cv2.bitwise_or(color_mask, edges)\n","\n","    kernel = np.ones((5, 5), np.uint8)\n","    final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_CLOSE, kernel)\n","    final_mask = cv2.dilate(final_mask, kernel, iterations=1)\n","\n","    return cv2.bitwise_and(image, image, mask=final_mask)\n","\n","def center_on_black(image):\n","    black_bg = np.zeros((TARGET_SIZE, TARGET_SIZE, 3), dtype=np.uint8)\n","    h, w = image.shape[:2]\n","    scale = min(TARGET_SIZE / h, TARGET_SIZE / w) * 0.8\n","    new_h, new_w = int(h * scale), int(w * scale)\n","    resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)\n","    y_offset = (TARGET_SIZE - new_h) // 2\n","    x_offset = (TARGET_SIZE - new_w) // 2\n","    black_bg[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized\n","    return black_bg\n","\n","# mixed를 max_samples까지 늘릴 때 사용할 특수 증강\n","def special_augment_for_mixed(image):\n","    h, w = image.shape[:2]\n","    center = (w // 2, h // 2)\n","    transforms = [\n","        lambda img: cv2.GaussianBlur(img, (5, 5), 1.5),\n","        lambda img: cv2.resize(img, None, fx=np.random.uniform(1.3, 1.7), fy=np.random.uniform(1.3, 1.7)),\n","        lambda img: cv2.warpAffine(img, cv2.getRotationMatrix2D(center, np.random.uniform(-15, 15), 1.0), (w, h))\n","    ]\n","    return random.choice(transforms)(image)\n","\n","# 다른 클래스가 부족할 경우 사용할 단순 증강\n","def simple_augment(image):\n","    transforms = [\n","        lambda img: cv2.flip(img, np.random.choice([-1, 0, 1])),\n","        lambda img: cv2.convertScaleAbs(img, alpha=np.random.uniform(0.8, 1.2), beta=np.random.randint(-30, 30)),\n","        lambda img: cv2.GaussianBlur(img, (3, 3), 0),\n","        lambda img: cv2.resize(img, None, fx=np.random.uniform(1.1,1.3), fy=np.random.uniform(1.1,1.3))\n","    ]\n","    return random.choice(transforms)(image)\n","\n","# 최종 증강 기법: 여러개 중 5개를 랜덤으로 선택, 각각 하나씩 적용\n","def final_augment_image(image):\n","    h, w = image.shape[:2]\n","    center = (w // 2, h // 2)\n","    transforms = {\n","        'rotation': lambda img: cv2.warpAffine(img, cv2.getRotationMatrix2D(center, np.random.randint(-30, 30), 1.0), (w, h)),\n","        'brightness': lambda img: cv2.convertScaleAbs(img, alpha=np.random.uniform(0.7, 1.3), beta=np.random.randint(-50, 50)),\n","        'flip': lambda img: cv2.flip(img, np.random.choice([-1, 0, 1])),\n","        'contrast': lambda img: cv2.convertScaleAbs(img, alpha=np.random.uniform(0.5, 1.5)),\n","        'zoom': lambda img: cv2.resize(img, None, fx=np.random.uniform(1.2, 1.5), fy=np.random.uniform(1.2, 1.5)),\n","        'blur': lambda img: cv2.GaussianBlur(img, (5, 5), 0),\n","        'shear': lambda img: cv2.warpAffine(img, cv2.getRotationMatrix2D(center, np.random.uniform(-10, 10), 1.0), (w, h))\n","    }\n","\n","    # 5개의 랜덤 증강 기법 선택\n","    chosen_keys = random.sample(list(transforms.keys()), 5)\n","    augmented_results = []\n","    for key in chosen_keys:\n","        aug_img = transforms[key](image)\n","        augmented_results.append((key, aug_img))\n","    return augmented_results\n","\n","def preprocess_and_select(train=True):\n","    input_dir = TRAIN_DIR if train else TEST_DIR\n","    output_dir = os.path.join(PREPROCESS_DIR, 'train' if train else 'test')\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # 원본 이미지 카운트\n","    class_counts = {}\n","    for filename in os.listdir(input_dir):\n","        if filename.endswith('.jpg'):\n","            class_name = filename.split('_')[0]\n","            class_counts[class_name] = class_counts.get(class_name, 0) + 1\n","\n","    max_samples = max(class_counts.values())\n","    print(f\"{'Training' if train else 'Test'} max samples per class: {max_samples}\")\n","\n","    for class_name, count in class_counts.items():\n","        class_dir = os.path.join(output_dir, class_name)\n","        os.makedirs(class_dir, exist_ok=True)\n","\n","        class_files = [f for f in os.listdir(input_dir) if f.startswith(class_name) and f.endswith('.jpg')]\n","        class_files.sort()\n","\n","        # 만약 count > max_samples면 max_samples까지만 사용 (학습 데이터일 경우)\n","        if train and count > max_samples:\n","            class_files = class_files[:max_samples]\n","\n","        processed_images = []\n","        # 전처리 진행\n","        for filename in tqdm(class_files, desc=f\"Preprocessing {class_name}\", leave=False):\n","            image_path = os.path.join(input_dir, filename)\n","            image = cv2.imread(image_path)\n","            if image is None:\n","                continue\n","            processed = remove_background(image)\n","            processed = center_on_black(processed)\n","            processed_images.append(processed)\n","\n","        # 부족한 경우 증강해서 max_samples 맞추기\n","        if train:\n","            if class_name == 'mixed' and len(processed_images) < max_samples:\n","                needed = max_samples - len(processed_images)\n","                for _ in range(needed):\n","                    base_img = random.choice(processed_images)\n","                    processed_images.append(special_augment_for_mixed(base_img))\n","            elif class_name != 'mixed' and len(processed_images) < max_samples:\n","                needed = max_samples - len(processed_images)\n","                for _ in range(needed):\n","                    base_img = random.choice(processed_images)\n","                    processed_images.append(simple_augment(base_img))\n","\n","            # 최종적으로 max_samples 만족\n","            assert len(processed_images) == max_samples, f\"{class_name} does not have {max_samples} images after preprocessing.\"\n","\n","        # test일 경우는 증강 없음, 그냥 처리된 이미지만 저장\n","        # train일 경우 max_samples개 이미지 저장 (증강 전 상태)\n","        for i, img in enumerate(processed_images):\n","            save_name = f\"{class_name}_{i+1}.jpg\"\n","            cv2.imwrite(os.path.join(class_dir, save_name), img)\n","\n","def final_augmentation():\n","    \"\"\"\n","    모든 학습 클래스에 대해:\n","    이미 각 클래스당 max_samples개 이미지를 갖춤.\n","    각 이미지당 5개 증강 기법을 랜덤 선택, 각각 1장씩 총 5장 증강 이미지 생성\n","    원본 1장 + 증강 5장 = 총 6장\n","    \"\"\"\n","    train_dir = os.path.join(PREPROCESS_DIR, 'train')\n","    for class_name in os.listdir(train_dir):\n","        class_path = os.path.join(train_dir, class_name)\n","        if not os.path.isdir(class_path):\n","            continue\n","        images = [f for f in os.listdir(class_path) if f.endswith('.jpg') and '_aug' not in f]\n","        images.sort()\n","\n","        for filename in tqdm(images, desc=f\"Final Augmenting {class_name}\", leave=False):\n","            img_path = os.path.join(class_path, filename)\n","            img = cv2.imread(img_path)\n","            if img is None:\n","                continue\n","            aug_imgs = final_augment_image(img)\n","            base_name = os.path.splitext(filename)[0]\n","\n","            # 선택된 증강 기법 출력 (옵션)\n","            chosen_transforms = [t[0] for t in aug_imgs]\n","            print(f\"{filename} -> chosen transforms: {chosen_transforms}\")\n","\n","            for (transform_name, aimg) in aug_imgs:\n","                aug_path = os.path.join(class_path, f\"{base_name}_aug_{transform_name}.jpg\")\n","                cv2.imwrite(aug_path, aimg)\n","\n","    # 최종 개수 출력\n","    for class_name in os.listdir(train_dir):\n","        cpath = os.path.join(train_dir, class_name)\n","        if os.path.isdir(cpath):\n","            final_count = len([f for f in os.listdir(cpath) if f.endswith('.jpg')])\n","            print(f\"Class {class_name} final count: {final_count}\")\n","\n","def plot_examples_for_mixed():\n","    class_dir = os.path.join(PREPROCESS_DIR, 'train', 'mixed')\n","    if not os.path.exists(class_dir):\n","        print(\"No mixed class directory found.\")\n","        return\n","    images = [f for f in os.listdir(class_dir) if f.endswith('.jpg') and '_aug' not in f]\n","    if not images:\n","        print(\"No images found in mixed class.\")\n","        return\n","    img_path = os.path.join(class_dir, images[0])\n","    img = cv2.imread(img_path)\n","    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    aug_imgs = final_augment_image(img)\n","\n","    plt.figure(figsize=(15, 3))\n","    plt.subplot(1, len(aug_imgs) + 1, 1)\n","    plt.imshow(img_rgb)\n","    plt.title(\"Original\")\n","    plt.axis('off')\n","\n","    for i, (name, aimg) in enumerate(aug_imgs, 1):\n","        plt.subplot(1, len(aug_imgs) + 1, i+1)\n","        plt.imshow(cv2.cvtColor(aimg, cv2.COLOR_BGR2RGB))\n","        plt.title(name)  # 증강 기법 이름 표시\n","        plt.axis('off')\n","    plt.show()\n","\n","if __name__ == \"__main__\":\n","    create_directories()\n","\n","    print(\"\\nPreprocessing training images...\")\n","    preprocess_and_select(train=True)\n","\n","    print(\"\\nPreprocessing test images (no augmentation)...\")\n","    preprocess_and_select(train=False)\n","\n","    print(\"\\nFinal augmentation for training images...\")\n","    final_augmentation()\n","\n","    print(\"\\nShowing examples for mixed class final augmentation...\")\n","    plot_examples_for_mixed()\n","\n"],"metadata":{"id":"cozHu00k4ipP"},"execution_count":null,"outputs":[]}]}